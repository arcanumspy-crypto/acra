# Pipeline Profissional de Clonagem de Voz

## ğŸ“‹ VisÃ£o Geral

Este pipeline replica o processo profissional do **Fish AI** para clonagem de voz:

1. **Processa o Ã¡udio** (prÃ©-processamento)
2. **Extrai o embedding da voz** (caracterÃ­sticas acÃºsticas)
3. **Ajusta sotaque e timbre** (via embeddings combinados)
4. **Usa servidor treinado** (Fish API ou modelo local)
5. **Aplica correÃ§Ãµes internas** (validaÃ§Ã£o)
6. **Roda no modelo mais atualizado** (s1 ou speech-1.5)
7. **Faz alinhamento de espectrograma** (opcional)

---

## ğŸš€ InstalaÃ§Ã£o

### 1. DependÃªncias Python (Worker)

**âš ï¸ IMPORTANTE:** Python precisa estar instalado primeiro!

**Verificar se Python estÃ¡ instalado:**
```powershell
python --version
# ou
py --version
```

**Se nÃ£o estiver instalado, veja:** `INSTALACAO_PYTHON_WINDOWS.md`

**Instalar dependÃªncias:**
```powershell
cd workers
python -m pip install -r requirements.txt
```

**Ou instalar manualmente:**

```powershell
python -m pip install librosa soundfile noisereduce resemblyzer numpy scipy requests python-dotenv
```

**Nota no Windows:** Use `python -m pip` ao invÃ©s de apenas `pip` se `pip` nÃ£o for reconhecido.

**Opcional (para ECAPA-TDNN):**

```bash
pip install speechbrain torch torchaudio
```

### 2. DependÃªncias Node.js (Backend)

```bash
npm install formidable node-fetch@2 axios dotenv
```

### 3. VariÃ¡veis de Ambiente

Crie arquivo `.env.local`:

```env
# Fish API
FISH_AUDIO_API_KEY=your_fish_api_key
FISH_AUDIO_API_URL=https://api.fish.audio

# Supabase
NEXT_PUBLIC_SUPABASE_URL=your_supabase_url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key

# Storage
STORAGE_BASE_URL=https://your-project.supabase.co

# Queue (opcional - para produÃ§Ã£o)
REDIS_URL=redis://localhost:6379
DATABASE_URL=postgres://user:pass@localhost:5432/db
```

---

## ğŸ“ Estrutura de Arquivos

```
ej-swipefile/
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”‚   â”œâ”€â”€ preprocess_and_embed.py      # PrÃ©-processamento + embedding
â”‚   â”œâ”€â”€ build_voice.py                # Worker pipeline completo
â”‚   â”œâ”€â”€ validate_generation.py        # ValidaÃ§Ã£o de similaridade
â”‚   â”œâ”€â”€ audio_preprocessor.py         # PrÃ©-processamento avanÃ§ado
â”‚   â”œâ”€â”€ voice_embedding_extractor.py  # ExtraÃ§Ã£o de embeddings
â”‚   â””â”€â”€ voice_pipeline.py             # Pipeline completo
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/api/voices/
â”‚   â”‚   â”œâ”€â”€ upload/route.ts           # Upload de Ã¡udios
â”‚   â”‚   â”œâ”€â”€ create-model/route.ts     # Criar modelo
â”‚   â”‚   â””â”€â”€ generate/route.ts        # Gerar Ã¡udio
â”‚   â”‚
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ storage.ts                # Helpers de storage
â”‚       â”œâ”€â”€ queue.ts                 # Helpers de queue
â”‚       â”œâ”€â”€ db-voice.ts              # Helpers de banco
â”‚       â””â”€â”€ voice-validation-advanced.ts  # ValidaÃ§Ã£o avanÃ§ada
â”‚
â””â”€â”€ PIPELINE_CLONAGEM_VOZ.md         # Esta documentaÃ§Ã£o
```

---

## ğŸ”„ Fluxo do Pipeline

### Etapa 1: Upload de Ãudios

**Endpoint:** `POST /api/voices/upload`

```typescript
const formData = new FormData()
formData.append('name', 'Minha Voz')
formData.append('audioCount', '2')
formData.append('audio0', audioFile1)
formData.append('audio1', audioFile2)
formData.append('transcripts', JSON.stringify(['OlÃ¡', 'Como vai?']))

const response = await fetch('/api/voices/upload', {
  method: 'POST',
  body: formData
})
```

**O que acontece:**
1. Valida autenticaÃ§Ã£o
2. Salva Ã¡udios no Supabase Storage
3. Cria job na fila para processamento
4. Retorna `jobId` e URLs dos Ã¡udios

---

### Etapa 2: Processamento (Worker Python)

**Worker:** `workers/build_voice.py`

```bash
# Executar worker manualmente (ou via queue)
python workers/build_voice.py
```

**O que acontece:**
1. Baixa Ã¡udios do storage
2. PrÃ©-processa cada Ã¡udio:
   - ConversÃ£o para mono
   - Resample para 24kHz
   - ReduÃ§Ã£o de ruÃ­do
   - NormalizaÃ§Ã£o RMS
   - Trim de silÃªncio
3. Extrai embeddings usando Resemblyzer
4. Combina embeddings (ajuste de sotaque/timbre)
5. Cria modelo na Fish API (ou local)
6. Salva `model_id` no banco

---

### Etapa 3: CriaÃ§Ã£o de Modelo

**Endpoint:** `POST /api/voices/create-model`

```typescript
const response = await fetch('/api/voices/create-model', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    name: 'Minha Voz',
    urls: ['https://...audio1.wav', 'https://...audio2.wav'],
    transcripts: ['OlÃ¡', 'Como vai?']
  })
})
```

**O que acontece:**
1. Baixa Ã¡udios das URLs
2. Converte para base64
3. Chama Fish API `/v1/models`
4. Salva `model_id` no banco

---

### Etapa 4: GeraÃ§Ã£o de Ãudio

**Endpoint:** `POST /api/voices/generate`

```typescript
const response = await fetch('/api/voices/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    modelId: 'model-id-from-fish',
    text: 'OlÃ¡, este Ã© um teste de voz.',
    params: {
      format: 'mp3',
      speed: 1.0,
      volume: 0,
      temperature: 0.9,
      top_p: 0.9,
      model: 's1'  // ou 'speech-1.5'
    }
  })
})
```

**O que acontece:**
1. Busca `model_id` no banco
2. Chama Fish API `/v1/tts`
3. Recebe Ã¡udio gerado
4. Salva no storage (opcional)
5. Retorna base64 ou URL

---

### Etapa 5: ValidaÃ§Ã£o

**Script:** `workers/validate_generation.py`

```bash
python workers/validate_generation.py \
  --reference path/to/reference.emb.json \
  --generated path/to/generated.wav \
  --threshold 0.82
```

**O que acontece:**
1. Carrega embedding de referÃªncia
2. Extrai embedding do Ã¡udio gerado
3. Calcula similaridade coseno
4. Aplica thresholds:
   - `>= 0.82`: âœ… OK
   - `0.75-0.82`: âš ï¸ Revisar
   - `< 0.75`: âŒ Rejeitar

---

## ğŸ§ª Testando o Pipeline

### 1. PrÃ©-processar Ãudio Individual

```bash
python workers/preprocess_and_embed.py \
  --input "/path/to/audio.wav" \
  --out "/path/to/output.wav" \
  --target-sr 24000
```

**SaÃ­da:**
- `output.wav` - Ãudio processado
- `output.wav.emb.json` - Embedding em JSON

---

### 2. Testar Upload

```bash
curl -X POST http://localhost:3000/api/voices/upload \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -F "name=Teste" \
  -F "audioCount=2" \
  -F "audio0=@audio1.wav" \
  -F "audio1=@audio2.wav"
```

---

### 3. Testar CriaÃ§Ã£o de Modelo

```bash
curl -X POST http://localhost:3000/api/voices/create-model \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Voz Teste",
    "urls": ["https://...audio1.wav", "https://...audio2.wav"],
    "transcripts": ["OlÃ¡", "Como vai?"]
  }'
```

---

### 4. Testar GeraÃ§Ã£o

```bash
curl -X POST http://localhost:3000/api/voices/generate \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "modelId": "model-id-from-fish",
    "text": "OlÃ¡, este Ã© um teste de voz.",
    "params": {
      "format": "mp3",
      "model": "s1"
    }
  }'
```

---

## ğŸ“Š Thresholds de ValidaÃ§Ã£o

| Similaridade | Status | AÃ§Ã£o |
|-------------|--------|------|
| `>= 0.82` | âœ… OK | Aceitar |
| `0.75 - 0.82` | âš ï¸ Revisar | Reprocessar com ajustes |
| `< 0.75` | âŒ Rejeitar | Falhou validaÃ§Ã£o |

**ParÃ¢metros para reprocessamento:**
- Reduzir `temperature` (0.9 â†’ 0.7)
- Reduzir `top_p` (0.9 â†’ 0.7)
- Usar modelo `s1` (garantir)
- Usar Ã¡udio de referÃªncia mais longo

---

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Usar ECAPA-TDNN (mais preciso, requer GPU)

```python
# Em voice_embedding_extractor.py
extractor = VoiceEmbeddingExtractor(model_type="ecapa-tdnn")
```

### Treinar Modelo Local (VITS/Coqui TTS)

```python
# Em build_voice.py
pipeline = VoiceCloningPipeline(use_fish_api=False)
# Requer GPU e configuraÃ§Ã£o adicional
```

### Forced Alignment (Opcional)

```bash
# Instalar Montreal Forced Aligner
pip install montreal-forced-alignment
```

---

## ğŸ“ Notas Importantes

1. **Qualidade dos Ãudios:**
   - MÃ­nimo: 2-3 Ã¡udios de 20-50 segundos cada
   - Ambiente consistente (mesmo microfone)
   - Sem ruÃ­do de fundo

2. **TranscriÃ§Ãµes:**
   - Melhoram significativamente a qualidade
   - Devem corresponder exatamente ao Ã¡udio

3. **Modelo Fish API:**
   - Use `s1` para preservar gÃªnero/timbre/sotaque
   - `speech-1.5` para qualidade mÃ¡xima

4. **Reprocessamento:**
   - Se validaÃ§Ã£o falhar, ajuste parÃ¢metros
   - MÃ¡ximo 3 tentativas
   - Marcar para revisÃ£o humana se persistir

---

## ğŸ› Troubleshooting

### Erro: "Bucket nÃ£o encontrado"
- Crie bucket `voice-clones` no Supabase Storage
- Configure permissÃµes RLS

### Erro: "FISH_AUDIO_API_KEY nÃ£o configurada"
- Configure variÃ¡vel no `.env.local`
- Reinicie servidor Next.js

### Erro: "Similaridade baixa"
- Verifique se Ã¡udios sÃ£o da mesma voz
- Aumente quantidade de Ã¡udios de referÃªncia
- Use transcriÃ§Ãµes precisas

### Erro: "Model ID nÃ£o encontrado"
- Verifique resposta da Fish API
- Use fallback para modelo local se necessÃ¡rio

---

## ğŸ“š ReferÃªncias

- [Fish Audio API Docs](https://docs.fish.audio)
- [Resemblyzer](https://github.com/resemble-ai/Resemblyzer)
- [SpeechBrain](https://speechbrain.github.io/)
- [Librosa](https://librosa.org/)

---

## âœ… Checklist de ImplementaÃ§Ã£o

- [x] Scripts Python de prÃ©-processamento
- [x] ExtraÃ§Ã£o de embeddings
- [x] Worker pipeline completo
- [x] Endpoints Next.js (upload, create-model, generate)
- [x] ValidaÃ§Ã£o de similaridade
- [x] Helpers de storage/queue/db
- [x] DocumentaÃ§Ã£o completa

**PrÃ³ximos passos:**
- [ ] Integrar com queue real (Redis/Bull)
- [ ] Adicionar reprocessamento automÃ¡tico
- [ ] Implementar validaÃ§Ã£o de gÃªnero/sotaque
- [ ] Adicionar monitoramento/logs

---

**Desenvolvido seguindo o pipeline profissional do Fish AI** ğŸ¯

